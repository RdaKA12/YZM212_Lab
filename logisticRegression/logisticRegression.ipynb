{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Projesi\n",
    "\n",
    "\n",
    "**Kodlar:**\n",
    "1. Veri Ön İşleme (Preprocessing)\n",
    "2. Scikit-Learn ile Logistic Regression Modeli\n",
    "3. Custom Logistic Regression Modeli (Optimizasyonlar ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal veri seti sütunları:\n",
      "['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "\n",
      "Orijinal veri seti eksik değerler:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Fare             0\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "Orijinal veri setinin ilk 5 satırı:\n",
      "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0            1         0       3    male  22.0      1      0   7.2500        S\n",
      "1            2         1       1  female  38.0      1      0  71.2833        C\n",
      "2            3         1       3  female  26.0      0      0   7.9250        S\n",
      "3            4         1       1  female  35.0      1      0  53.1000        S\n",
      "4            5         0       3    male  35.0      0      0   8.0500        S\n",
      "\n",
      "Gereksiz sütunlar çıkarıldıktan sonra sütunlar:\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "\n",
      "Numerik sütunlarda eksik değerler doldurulduktan sonra (Age, Fare):\n",
      "Age     0\n",
      "Fare    0\n",
      "dtype: int64\n",
      "\n",
      "Kategorik sütunlarda eksik değerler doldurulduktan sonra:\n",
      "Sex         0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "\n",
      "Kategorik değişkenler dummy değişkenlere dönüştürüldükten sonra sütunlar:\n",
      "['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
      "\n",
      "Numerik sütunlar ölçeklendirildikten sonra (Age, Fare) tanımlayıcı istatistikler:\n",
      "                Age          Fare\n",
      "count  8.910000e+02  8.910000e+02\n",
      "mean   2.272780e-16  3.987333e-18\n",
      "std    1.000562e+00  1.000562e+00\n",
      "min   -2.224156e+00 -6.484217e-01\n",
      "25%   -5.657365e-01 -4.891482e-01\n",
      "50%   -1.046374e-01 -3.573909e-01\n",
      "75%    4.333115e-01 -2.424635e-02\n",
      "max    3.891554e+00  9.667167e+00\n",
      "\n",
      "İşlenmiş veri setinin ilk 5 satırı:\n",
      "   Survived  Pclass       Age  SibSp  Parch      Fare  Sex_male  Embarked_Q  \\\n",
      "0         0       3 -0.565736      1      0 -0.502445      True       False   \n",
      "1         1       1  0.663861      1      0  0.786845     False       False   \n",
      "2         1       3 -0.258337      0      0 -0.488854     False       False   \n",
      "3         1       1  0.433312      1      0  0.420730     False       False   \n",
      "4         0       3  0.433312      0      0 -0.486337      True       False   \n",
      "\n",
      "   Embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n",
      "\n",
      "İşlenmiş veri seti Titanic-Dataset-Processed.xlsx dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(input_file: str, output_file: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Excel dosyasından Titanic veri setini okuyup; gereksiz sütunları kaldırır, eksik değerleri doldurur, \n",
    "    kategorik sütunları dummy değişkenlere dönüştürür ve numerik sütunları ölçeklendirir.\n",
    "    İşlenmiş DataFrame'i döndürür ve istenirse dosyaya kaydeder.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    print(\"Orijinal veri seti sütunları:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nOrijinal veri seti eksik değerler:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nOrijinal veri setinin ilk 5 satırı:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Gereksiz sütunların çıkarılması\n",
    "    columns_to_drop = ['Cabin', 'Name', 'Ticket', 'PassengerId']\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "    print(\"\\nGereksiz sütunlar çıkarıldıktan sonra sütunlar:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Numerik sütunlarda eksik değerlerin doldurulması\n",
    "    if 'Age' in df.columns:\n",
    "        df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    if 'Fare' in df.columns:\n",
    "        df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    print(\"\\nNumerik sütunlarda eksik değerler doldurulduktan sonra (Age, Fare):\")\n",
    "    print(df[['Age', 'Fare']].isnull().sum())\n",
    "    \n",
    "    # Kategorik sütunlarda eksik değerlerin doldurulması\n",
    "    if 'Embarked' in df.columns:\n",
    "        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    print(\"\\nKategorik sütunlarda eksik değerler doldurulduktan sonra:\")\n",
    "    print(df.select_dtypes(include='object').isnull().sum())\n",
    "    \n",
    "    # Kategorik değişkenlerin dummy değişkenlere dönüştürülmesi\n",
    "    categorical_cols = ['Sex', 'Embarked']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    print(\"\\nKategorik değişkenler dummy değişkenlere dönüştürüldükten sonra sütunlar:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Numerik sütunların ölçeklenmesi\n",
    "    scaler = StandardScaler()\n",
    "    for col in ['Age', 'Fare']:\n",
    "        if col in df.columns:\n",
    "            df[col] = scaler.fit_transform(df[[col]])\n",
    "    print(\"\\nNumerik sütunlar ölçeklendirildikten sonra (Age, Fare) tanımlayıcı istatistikler:\")\n",
    "    print(df[['Age', 'Fare']].describe())\n",
    "    \n",
    "    print(\"\\nİşlenmiş veri setinin ilk 5 satırı:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    if output_file:\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"\\nİşlenmiş veri seti {output_file} dosyasına kaydedildi.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocessing'i çalıştır\n",
    "df_processed = preprocess_data(\"Titanic-Dataset.xlsx\", \"Titanic-Dataset-Processed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn ile Logistic Regression\n",
    "\n",
    "Aşağıdaki kod, işlenmiş veri setini kullanarak Scikit-Learn'ün `LogisticRegression` sınıfı ile modeli eğitir, tahmin yapar ve sonuçları değerlendirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.0041 seconds\n",
      "Prediction time: 0.0007 seconds\n",
      "\n",
      "Confusion Matrix:\n",
      "[[90 15]\n",
      " [19 55]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       105\n",
      "           1       0.79      0.74      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "def run_scikit_lr(df):\n",
    "    X = df.drop(\"Survived\", axis=1)\n",
    "    y = df[\"Survived\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    print(\"Training time: {:.4f} seconds\".format(training_time))\n",
    "    print(\"Prediction time: {:.4f} seconds\".format(prediction_time))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Scikit-Learn modelini çalıştır\n",
    "run_scikit_lr(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Logistic Regression\n",
    "\n",
    "Aşağıdaki kod custom Logistic Regression modelini gradient descent kullanarak eğitir.\n",
    "Bu modelde erken durdurma (early stopping) ve L2 regularizasyon vardır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb821d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Custom Logistic Regression -----\n",
      "Iteration 0: Cost = 0.6931271807599426\n",
      "Iteration 100: Cost = 0.6141685388556968\n",
      "Iteration 200: Cost = 0.5979830977239932\n",
      "Iteration 300: Cost = 0.5863592372088651\n",
      "Iteration 400: Cost = 0.5768961394274994\n",
      "Iteration 500: Cost = 0.5688638553207505\n",
      "Iteration 600: Cost = 0.5618711666674641\n",
      "Iteration 700: Cost = 0.5556813364941928\n",
      "Iteration 800: Cost = 0.5501383662838315\n",
      "Iteration 900: Cost = 0.5451321637809293\n",
      "\n",
      "Custom model final cost: 0.540624327638365\n",
      "Custom model training time: 0.0287 seconds\n",
      "Custom model prediction time: 0.0000 seconds\n",
      "\n",
      "Custom model Confusion Matrix:\n",
      "[[103   2]\n",
      " [ 47  27]]\n",
      "\n",
      "Custom model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.98      0.81       105\n",
      "         1.0       0.93      0.36      0.52        74\n",
      "\n",
      "    accuracy                           0.73       179\n",
      "   macro avg       0.81      0.67      0.67       179\n",
      "weighted avg       0.79      0.73      0.69       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(theta, X, y, reg_lambda=0):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    epsilon = 1e-5\n",
    "    cost = - (1/m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "    # L2 regularizasyon (bias hariç)\n",
    "    reg_term = (reg_lambda / (2*m)) * np.sum(theta[1:] ** 2)\n",
    "    cost += reg_term\n",
    "    grad = (1/m) * X.T.dot(h - y)\n",
    "    grad[1:] += (reg_lambda / m) * theta[1:]\n",
    "    return cost, grad\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations, tol=1e-6, reg_lambda=0):\n",
    "    cost_history = []\n",
    "    for i in range(iterations):\n",
    "        cost, grad = cost_function(theta, X, y, reg_lambda)\n",
    "        theta = theta - learning_rate * grad\n",
    "        cost_history.append(cost)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost = {cost}\")\n",
    "        if i > 0 and abs(cost_history[-2] - cost_history[-1]) < tol:\n",
    "            print(f\"Convergence reached at iteration {i}\")\n",
    "            break\n",
    "    return theta, cost_history\n",
    "\n",
    "def run_custom_lr(df):\n",
    "    print(\"\\n----- Custom Logistic Regression -----\")\n",
    "    # Tüm sütunları numeric (float) tipinde olduğundan emin olarak NumPy array'e dönüştür\n",
    "    X_np = df.drop(\"Survived\", axis=1).astype(float).values\n",
    "    y_np = df[\"Survived\"].astype(float).values\n",
    "    \n",
    "    # Bias terimini ekle\n",
    "    X_np = np.hstack([np.ones((X_np.shape[0], 1)), X_np])\n",
    "    \n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "    \n",
    "    theta = np.zeros(X_train_np.shape[1])\n",
    "    learning_rate = 0.01\n",
    "    iterations = 1000\n",
    "    reg_lambda = 0.1  # L2 regularizasyon parametresi\n",
    "    \n",
    "    start_time = time.time()\n",
    "    theta, cost_history = gradient_descent(X_train_np, y_train_np, theta, learning_rate, iterations, tol=1e-6, reg_lambda=reg_lambda)\n",
    "    training_time_custom = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nCustom model final cost:\", cost_history[-1])\n",
    "    print(\"Custom model training time: {:.4f} seconds\".format(training_time_custom))\n",
    "    \n",
    "    def predict(theta, X):\n",
    "        return (sigmoid(X.dot(theta)) >= 0.5).astype(int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred_custom = predict(theta, X_test_np)\n",
    "    prediction_time_custom = time.time() - start_time\n",
    "    \n",
    "    print(\"Custom model prediction time: {:.4f} seconds\".format(prediction_time_custom))\n",
    "    print(\"\\nCustom model Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_np, y_pred_custom))\n",
    "    print(\"\\nCustom model Classification Report:\")\n",
    "    print(classification_report(y_test_np, y_pred_custom))\n",
    "\n",
    "# Custom modeli çalıştır\n",
    "run_custom_lr(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonuç\n",
    "\n",
    "Bu notebook, veri ön işleme, Scikit-Learn Logistic Regression ve custom (optimizasyonlu) Logistic Regression model eğitim adımlarını içerir.\n",
    "Eğitim ve tahmin süreleri, karmaşıklık matrisi ve sınıflandırma raporları çıktı olarak gösterilecektir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
